{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Case Study or Pansas in NLP domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Our taks is to make an model who will predict the sentiment  of texual data `"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in modeling:-\n",
    "1. Data gathering \n",
    "2. Data cleaning\n",
    "3. Data preprocessing\n",
    "4. EDA\n",
    "5. Make features (additional colm)\n",
    "6. vectorization (text to no transformation)\n",
    "7. Modeling\n",
    "8. Evaluation\n",
    "9. Deployment\n",
    "10. Monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Data gathering\n",
    "1. API\n",
    "2. Webscrapping\n",
    "3. kagle etc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data cleaning :-\n",
    "1. lowercasing \n",
    "2. remove leading and trailing spacing\n",
    "3. remove html tags\n",
    "4. expand abbrevaiations\n",
    "5. spelling correction\n",
    "6. punctuations\n",
    "7. remove special charachters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Data Preprocessing:-\n",
    "1. tokenization (sentance ko smallest parts me divide karte hai mostly words)\n",
    "2. stopword removal (jo sentiment me help nai karte apne kam ke nai ex: I could have - no useful)\n",
    "3. stemming (ex- dance,danced,dances ==> dance!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()       #no missing value nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()   #we have duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is very huge 50K will take lots of time for loading for now will take only 10k rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[ : 10000]\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Data cleaning now -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()      #not all in lowecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove whitespaces\n",
    "df['review'] = df['review'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkark\\AppData\\Local\\Temp\\ipykernel_18092\\76541410.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review'] = df['review'].str.replace(r'<.*?>','')   #little regex knowledge required\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove html tags\n",
    "df['review'] = df['review'].str.replace(r'<.*?>','')   #little regex knowledge required\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"this show has to be my favorite out of all the 80's horror tv shows. like tales from the darkside, also from the same creators, this show is a rare gem. if you agree with me, please sign this petition i started, to get the word out for monsters and get it out on dvd. here is the petition address: www.petitiononline.com/19784444/petition.html some of my favorite episodes would have to be glim glim, and rain dance. i also loved the opening intro with the monster family. that used to creep me out! one of the things i would have to ask the dvd creators to include would be the organ sound heard right before where the commercial break would be. i don't know if any of you remember that part but that's one of the main things that brings back memories to me. i mean, come on! war of the worlds the tv series already has been released on dvd, so i say monsters, and also tales from the darkside, and friday the 13th the series should be released too! we the fans need to speak our minds! we need this awesome show on dvd so please spread the word!!!\",\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to check if a review contains an URL\n",
    "df[df['review'].str.contains(r\"https?://\\S+|www\\.\\S+\")].iloc[4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any url\n",
    "# http://campusx.in www.campusx.in www.campusx.co.in\n",
    "\n",
    "#Method 1\n",
    "\n",
    "def remove_url(data):\n",
    "    data=re.sub(r\"https?://\\S+|www\\.\\S+\",'',data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkark\\AppData\\Local\\Temp\\ipykernel_18092\\3196874022.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review'] = df['review'].str.replace(r\"https?://\\S+|www\\.\\S+\",'')\n"
     ]
    }
   ],
   "source": [
    "#Method 2\n",
    "# code to remove a URL\n",
    "df['review'] = df['review'].str.replace(r\"https?://\\S+|www\\.\\S+\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# code to check if a review contains an URL\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[df[\u001b[39m'\u001b[39;49m\u001b[39mreview\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mcontains(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps?://\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mS+|www\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mS+\u001b[39;49m\u001b[39m\"\u001b[39;49m)]\u001b[39m.\u001b[39;49miloc[\u001b[39m4\u001b[39;49m]\u001b[39m.\u001b[39mvalues  \u001b[39m#will give an error because ab remove ho gaya hai\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1625\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index by location index with a non-integer key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1624\u001b[0m \u001b[39m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1625\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_integer(key, axis)\n\u001b[0;32m   1627\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_ixs(key, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1557\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m len_axis \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m key \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msingle positional indexer is out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# code to check if a review contains an URL\n",
    "df[df['review'].str.contains(r\"https?://\\S+|www\\.\\S+\")].iloc[4].values  #will give an error because ab remove ho gaya hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding abbvr\n",
    "# expand \n",
    "\n",
    "import re\n",
    "\n",
    "def remove_abb(data):\n",
    "    data = re.sub(r\"he's\", \"he is\", data)\n",
    "    data = re.sub(r\"there's\", \"there is\", data)\n",
    "    data = re.sub(r\"We're\", \"We are\", data)\n",
    "    data = re.sub(r\"That's\", \"That is\", data)\n",
    "    data = re.sub(r\"won't\", \"will not\", data)\n",
    "    data = re.sub(r\"they're\", \"they are\", data)\n",
    "    data = re.sub(r\"Can't\", \"Cannot\", data)\n",
    "    data = re.sub(r\"wasn't\", \"was not\", data)\n",
    "    data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n",
    "    data= re.sub(r\"aren't\", \"are not\", data)\n",
    "    data = re.sub(r\"isn't\", \"is not\", data)\n",
    "    data = re.sub(r\"What's\", \"What is\", data)\n",
    "    data = re.sub(r\"haven't\", \"have not\", data)\n",
    "    data = re.sub(r\"hasn't\", \"has not\", data)\n",
    "    data = re.sub(r\"There's\", \"There is\", data)\n",
    "    data = re.sub(r\"He's\", \"He is\", data)\n",
    "    data = re.sub(r\"It's\", \"It is\", data)\n",
    "    data = re.sub(r\"You're\", \"You are\", data)\n",
    "    data = re.sub(r\"I'M\", \"I am\", data)\n",
    "    data = re.sub(r\"shouldn't\", \"should not\", data)\n",
    "    data = re.sub(r\"wouldn't\", \"would not\", data)\n",
    "    data = re.sub(r\"i'm\", \"I am\", data)\n",
    "    data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n",
    "    data = re.sub(r\"I'm\", \"I am\", data)\n",
    "    data = re.sub(r\"Isn't\", \"is not\", data)\n",
    "    data = re.sub(r\"Here's\", \"Here is\", data)\n",
    "    data = re.sub(r\"you've\", \"you have\", data)\n",
    "    data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n",
    "    data = re.sub(r\"we're\", \"we are\", data)\n",
    "    data = re.sub(r\"what's\", \"what is\", data)\n",
    "    data = re.sub(r\"couldn't\", \"could not\", data)\n",
    "    data = re.sub(r\"we've\", \"we have\", data)\n",
    "    data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n",
    "    data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n",
    "    data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n",
    "    data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n",
    "    data = re.sub(r\"who's\", \"who is\", data)\n",
    "    data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n",
    "    data = re.sub(r\"y'all\", \"you all\", data)\n",
    "    data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n",
    "    data = re.sub(r\"would've\", \"would have\", data)\n",
    "    data = re.sub(r\"it'll\", \"it will\", data)\n",
    "    data = re.sub(r\"we'll\", \"we will\", data)\n",
    "    data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n",
    "    data = re.sub(r\"We've\", \"We have\", data)\n",
    "    data = re.sub(r\"he'll\", \"he will\", data)\n",
    "    data = re.sub(r\"Y'all\", \"You all\", data)\n",
    "    data = re.sub(r\"Weren't\", \"Were not\", data)\n",
    "    data = re.sub(r\"Didn't\", \"Did not\", data)\n",
    "    data = re.sub(r\"they'll\", \"they will\", data)\n",
    "    data = re.sub(r\"they'd\", \"they would\", data)\n",
    "    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n",
    "    data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n",
    "    data = re.sub(r\"they've\", \"they have\", data)\n",
    "    data = re.sub(r\"i'd\", \"I would\", data)\n",
    "    data = re.sub(r\"should've\", \"should have\", data)\n",
    "    data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n",
    "    data = re.sub(r\"where's\", \"where is\", data)\n",
    "    data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n",
    "    data = re.sub(r\"we'd\", \"we would\", data)\n",
    "    data = re.sub(r\"i'll\", \"I will\", data)\n",
    "    data = re.sub(r\"weren't\", \"were not\", data)\n",
    "    data = re.sub(r\"They're\", \"They are\", data)\n",
    "    data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n",
    "    data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n",
    "    data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n",
    "    data = re.sub(r\"let's\", \"let us\", data)\n",
    "    data = re.sub(r\"it's\", \"it is\", data)\n",
    "    data = re.sub(r\"can't\", \"cannot\", data)\n",
    "    data = re.sub(r\"don't\", \"do not\", data)\n",
    "    data = re.sub(r\"you're\", \"you are\", data)\n",
    "    data = re.sub(r\"i've\", \"I have\", data)\n",
    "    data = re.sub(r\"that's\", \"that is\", data)\n",
    "    data = re.sub(r\"i'll\", \"I will\", data)\n",
    "    data = re.sub(r\"doesn't\", \"does not\",data)\n",
    "    data = re.sub(r\"i'd\", \"I would\", data)\n",
    "    data = re.sub(r\"didn't\", \"did not\", data)\n",
    "    data = re.sub(r\"ain't\", \"am not\", data)\n",
    "    data = re.sub(r\"you'll\", \"you will\", data)\n",
    "    data = re.sub(r\"I've\", \"I have\", data)\n",
    "    data = re.sub(r\"Don't\", \"do not\", data)\n",
    "    data = re.sub(r\"I'll\", \"I will\", data)\n",
    "    data = re.sub(r\"I'd\", \"I would\", data)\n",
    "    data = re.sub(r\"Let's\", \"Let us\", data)\n",
    "    data = re.sub(r\"you'd\", \"You would\", data)\n",
    "    data = re.sub(r\"It's\", \"It is\", data)\n",
    "    data = re.sub(r\"Ain't\", \"am not\", data)\n",
    "    data = re.sub(r\"Haven't\", \"Have not\", data)\n",
    "    data = re.sub(r\"Could've\", \"Could have\", data)\n",
    "    data = re.sub(r\"youve\", \"you have\", data)  \n",
    "    data = re.sub(r\"donå«t\", \"do not\", data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know you'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example working re module\n",
    "remove_abb(\"I don't know you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one of the other reviewers has mentioned that ...\n",
       "1    a wonderful little production. the filming tec...\n",
       "2    i thought this was a wonderful way to spend ti...\n",
       "3    basically there is a family where a little boy...\n",
       "4    petter mattei's \"love in the time of money\" is...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_abb)\n",
    "df['review'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love exercises at mountain'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "text = 'I love excrsises at moutnain'\n",
    "TextBlob(text).correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(text):\n",
    "    return TextBlob(text).correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mreview\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(spelling_correction)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m, in \u001b[0;36mspelling_correction\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspelling_correction\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m TextBlob(text)\u001b[39m.\u001b[39;49mcorrect()\u001b[39m.\u001b[39mstring\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\blob.py:609\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mtokenize\u001b[39m.\u001b[39mregexp_tokenize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+|[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    608\u001b[0m corrected \u001b[39m=\u001b[39m (Word(w)\u001b[39m.\u001b[39mcorrect() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens)\n\u001b[1;32m--> 609\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(corrected)\n\u001b[0;32m    610\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\blob.py:608\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[39m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[0;32m    607\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mtokenize\u001b[39m.\u001b[39mregexp_tokenize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+|[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 608\u001b[0m corrected \u001b[39m=\u001b[39m (Word(w)\u001b[39m.\u001b[39;49mcorrect() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens)\n\u001b[0;32m    609\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(corrected)\n\u001b[0;32m    610\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    137\u001b[0m     \u001b[39m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m Word(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspellcheck()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspellcheck\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    126\u001b[0m     \u001b[39m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[39m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m suggest(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\en\\__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msuggest\u001b[39m(w):\n\u001b[0;32m    121\u001b[0m     \u001b[39m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m spelling\u001b[39m.\u001b[39;49msuggest(w)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\_text.py:1399\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misdigit():\n\u001b[0;32m   1396\u001b[0m     \u001b[39mreturn\u001b[39;00m [(w, \u001b[39m1.0\u001b[39m)] \u001b[39m# 1.5\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known([w]) \\\n\u001b[0;32m   1398\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w)) \\\n\u001b[1;32m-> 1399\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit2(w)) \\\n\u001b[0;32m   1400\u001b[0m           \u001b[39mor\u001b[39;00m [w]\n\u001b[0;32m   1401\u001b[0m candidates \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(c, \u001b[39m0.0\u001b[39m), c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m candidates]\n\u001b[0;32m   1402\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39msum\u001b[39m(p \u001b[39mfor\u001b[39;00m p, word \u001b[39min\u001b[39;00m candidates) \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\_text.py:1376\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(e2 \u001b[39mfor\u001b[39;00m e1 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w) \u001b[39mfor\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(e1) \u001b[39mif\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\_text.py:1376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(e2 \u001b[39mfor\u001b[39;00m e1 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w) \u001b[39mfor\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit1(e1) \u001b[39mif\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\_text.py:1367\u001b[0m, in \u001b[0;36mSpelling._edit1\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[39m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \u001b[39m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[0;32m   1362\u001b[0m split \u001b[39m=\u001b[39m [(w[:i], w[i:]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(w) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[0;32m   1363\u001b[0m delete, transpose, replace, insert \u001b[39m=\u001b[39m (\n\u001b[0;32m   1364\u001b[0m     [a \u001b[39m+\u001b[39m b[\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m split \u001b[39mif\u001b[39;00m b],\n\u001b[0;32m   1365\u001b[0m     [a \u001b[39m+\u001b[39m b[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m b[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m b[\u001b[39m2\u001b[39m:] \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m split \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(b) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m],\n\u001b[0;32m   1366\u001b[0m     [a \u001b[39m+\u001b[39m c \u001b[39m+\u001b[39m b[\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m Spelling\u001b[39m.\u001b[39mALPHA \u001b[39mif\u001b[39;00m b],\n\u001b[1;32m-> 1367\u001b[0m     [a \u001b[39m+\u001b[39;49m c \u001b[39m+\u001b[39;49m b[\u001b[39m0\u001b[39;49m:] \u001b[39mfor\u001b[39;49;00m a, b \u001b[39min\u001b[39;49;00m split \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m Spelling\u001b[39m.\u001b[39;49mALPHA]\n\u001b[0;32m   1368\u001b[0m )\n\u001b[0;32m   1369\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(delete \u001b[39m+\u001b[39m transpose \u001b[39m+\u001b[39m replace \u001b[39m+\u001b[39m insert)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\_text.py:1367\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[39m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \u001b[39m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[0;32m   1362\u001b[0m split \u001b[39m=\u001b[39m [(w[:i], w[i:]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(w) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[0;32m   1363\u001b[0m delete, transpose, replace, insert \u001b[39m=\u001b[39m (\n\u001b[0;32m   1364\u001b[0m     [a \u001b[39m+\u001b[39m b[\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m split \u001b[39mif\u001b[39;00m b],\n\u001b[0;32m   1365\u001b[0m     [a \u001b[39m+\u001b[39m b[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m b[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m b[\u001b[39m2\u001b[39m:] \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m split \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(b) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m],\n\u001b[0;32m   1366\u001b[0m     [a \u001b[39m+\u001b[39m c \u001b[39m+\u001b[39m b[\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m Spelling\u001b[39m.\u001b[39mALPHA \u001b[39mif\u001b[39;00m b],\n\u001b[1;32m-> 1367\u001b[0m     [a \u001b[39m+\u001b[39m c \u001b[39m+\u001b[39m b[\u001b[39m0\u001b[39m:] \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m Spelling\u001b[39m.\u001b[39mALPHA]\n\u001b[0;32m   1368\u001b[0m )\n\u001b[0;32m   1369\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(delete \u001b[39m+\u001b[39m transpose \u001b[39m+\u001b[39m replace \u001b[39m+\u001b[39m insert)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['review'].apply(spelling_correction)   #do not run taking too much time 7 min I waited last time "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puctuation(text):\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "        if i in text:\n",
    "            text = text.replace(i,'')\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one of the other reviewers has mentioned that ...\n",
       "1    a wonderful little production the filming tech...\n",
       "2    i thought this was a wonderful way to spend ti...\n",
       "3    basically there is a family where a little boy...\n",
       "4    petter matteis love in the time of money is a ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_puctuation)\n",
    "df['review'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\kkark/nltk_data'\n    - 'c:\\\\Users\\\\kkark\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\kkark\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\kkark\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\kkark\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtokenized_review\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mreview\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(word_tokenize)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\kkark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\kkark/nltk_data'\n    - 'c:\\\\Users\\\\kkark\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\kkark\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\kkark\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\kkark\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df['tokenized_review'] = df['review'].apply(word_tokenize)   #not working  here in vscode "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f347c3a38a5b38569b7ec1709278d62e7c29be003839bb1ea9507b2f10b81935"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
